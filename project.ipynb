{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1rXgGW06919gXjOXzKphZzQ8lASaYSEk5","authorship_tag":"ABX9TyM5wLZzdm1vwkhIP/SXxTFV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2b3o80CEYRSC","collapsed":true},"outputs":[],"source":["!pip install snntorch"]},{"cell_type":"code","source":["import snntorch as snn\n","from snntorch import spikeplot as splt\n","from snntorch import spikegen\n","\n","import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from snntorch import utils"],"metadata":{"id":"toyu7sJoZk4R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training Parameters\n","batch_size=128\n","data_path='/tmp/data/fashion-mnist'\n","num_classes = 10  # fashion MNIST has 10 output classes\n","\n","dtype = torch.float\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","# print(device)"],"metadata":{"id":"meE5NQr2ZwZR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a transform\n","transform = transforms.Compose([\n","            transforms.Resize((28,28)),\n","            transforms.Grayscale(),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0,), (1,))])\n","\n","fmnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform)\n","fmnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transform)"],"metadata":{"id":"GzT32VlZaKPZ","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loader serves minibatches of batch_size\n","\n","train_loader = DataLoader(fmnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n","test_loader = DataLoader(fmnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"],"metadata":{"id":"X8ecJbPvahXP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Time to combine to build our model\n","class LeakySurrogate(nn.Module):\n","  def __init__(self, beta, threshold=1.0) -> None:\n","    super(LeakySurrogate, self).__init__()\n","\n","    # Initialize decay rate constant and threshold\n","    self.beta = beta\n","    self.threshold = threshold # R=1 to simplify\n","    self.spk_gradient = self.Arctan.apply\n","\n","  def forward(self, input_, mem):\n","    # Define forward pass\n","    spk = self.spk_gradient(mem-self.threshold) # spike = 0.0 or 1.0\n","    reset = (self.beta * spk * self.threshold).detach() # reset membrane potential\n","    mem = self.beta * mem + input_ - reset # remove reset before backward pass\n","    return spk, mem\n","\n","  # Autograd func\n","  @staticmethod\n","  class Arctan(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, mem):\n","      # Returns 1.0 if over threshold, 0.0 otherwise\n","      ctx.save_for_backward(mem)\n","      return (mem > 0).float()\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","      # Custom to avoid dead neuron\n","      (mem, ) = ctx.saved_tensors  # Retrieve\n","      grad_input = 1 / (1+(np.pi*mem).pow_(2)) * grad_output # Smoothing, modifying tensor in place for space efficiency\n","      return grad_input"],"metadata":{"id":"CoyBZViHZrTt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Network arch\n","num_inputs = 28*28\n","num_hidden = 1500\n","num_outputs = 10\n","\n","# Temporal dynamics\n","num_steps = 25\n","beta = 0.95"],"metadata":{"id":"Z0G6Mk7Naukb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define network\n","class Network(nn.Module):\n","  def __init__(self) -> None:\n","    super().__init__()\n","\n","    # Initialize layers\n","    self.fc1 = nn.Linear(num_inputs, num_hidden)  # transform input pixels\n","    self.lif1 = LeakySurrogate(beta)              # weighted input over time\n","    self.fc2 = nn.Linear(num_hidden, num_outputs) # transform output spikes\n","    self.lif2 = LeakySurrogate(beta)              # weighted spikes over time\n","\n","  def forward(self, x):\n","\n","    # At t=0\n","    mem1 = None\n","    mem2 = None\n","\n","    # Save final layer\n","    spk2_rec = []\n","    mem2_rec = []\n","\n","    for step in range(num_steps):\n","      # Apply layers\n","\n","      cur1 = self.fc1(x)\n","\n","      if mem1 is None:\n","        mem1 = torch.zeros_like(self.fc1(x))\n","      spk1, mem1 = self.lif1(cur1, mem1)\n","      # print(\"spk1 shape:\", spk1.shape)\n","\n","      cur2 = self.fc2(spk1)\n","\n","      if mem2 is None:\n","        mem2 = torch.zeros_like(self.fc2(spk1))\n","      spk2, mem2 = self.lif2(cur2, mem2)\n","\n","      # Save for recording\n","      spk2_rec.append(spk2)\n","      mem2_rec.append(mem2)\n","\n","    # Convert lists to tensors\n","    spk2_rec = torch.stack(spk2_rec, dim=0)\n","    mem2_rec = torch.stack(mem2_rec, dim=0)\n","    return spk2_rec, mem2_rec\n"],"metadata":{"id":"OGkm9Jn-a159"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load network onto cuda\n","net = Network().to(device)"],"metadata":{"id":"qXhHF85_a9G3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Accuracy metrics\n","\n","def print_batch_accuracy(data, targets, train=False):\n","    output, _ = net(data.view(batch_size, -1))\n","    _, idx = output.sum(dim=0).max(1)\n","    acc = np.mean((targets == idx).detach().cpu().numpy())\n","\n","    if train:\n","        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n","    else:\n","        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n","\n","def train_printer(\n","    data, targets, epoch,\n","    counter, iter_counter,\n","        loss_hist, test_loss_hist, test_data, test_targets):\n","    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n","    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n","    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n","    print_batch_accuracy(data, targets, train=True)\n","    print_batch_accuracy(test_data, test_targets, train=False)\n","    print(\"\\n\")"],"metadata":{"id":"CVIQTqDPbSea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loss func\n","loss = nn.CrossEntropyLoss()\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))"],"metadata":{"id":"4sxxDN5rbX42"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training params\n","num_epochs = 3\n","loss_hist = []\n","test_loss_hist = []\n","counter = 0"],"metadata":{"id":"f8DPsm99cdi_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net.train()\n","\n","# Outer training loop\n","for epoch in range(num_epochs):\n","    iter_counter = 0\n","    train_batch = iter(train_loader)\n","    total = 0\n","    correct = 0\n","\n","    # Minibatch training loop\n","    for data, targets in train_batch:\n","        data = data.to(device)\n","        targets = targets.to(device)\n","\n","        # forward pass\n","        spk_rec, mem_rec = net(data.view(batch_size, -1))\n","\n","        # initialize the loss & sum over time\n","        loss_val = torch.zeros((1), dtype=dtype, device=device)\n","        for step in range(num_steps):\n","            loss_val += loss(mem_rec[step], targets)\n","\n","        # Gradient calculation + weight update\n","        optimizer.zero_grad()\n","        loss_val.backward()\n","        optimizer.step()\n","\n","        # Store loss history for future plotting\n","        loss_hist.append(loss_val.item())\n","\n","        # Test set after each batch\n","        with torch.no_grad():\n","            net.eval()\n","            test_data, test_targets = next(iter(test_loader))\n","            test_data = test_data.to(device)\n","            test_targets = test_targets.to(device)\n","\n","            # Test set forward pass\n","            test_spk, test_mem = net(test_data.view(batch_size, -1))\n","\n","            # Test set accuracy\n","            _, predicted = test_spk.sum(dim=0).max(1)\n","            total += targets.size(0)\n","            correct += (predicted == test_targets).sum().item()\n","\n","            # Test set loss\n","            test_loss = torch.zeros((1), dtype=dtype, device=device)\n","            for step in range(num_steps):\n","                test_loss += loss(test_mem[step], test_targets)\n","            test_loss_hist.append(test_loss.item())\n","\n","            # Print train/test loss/accuracy\n","            if counter % 50 == 0:\n","                train_printer(\n","                    data, targets, epoch,\n","                    counter, iter_counter,\n","                    loss_hist, test_loss_hist,\n","                    test_data, test_targets)\n","            counter += 1\n","            iter_counter +=1\n","    print(f\"Total correctly classified test set images after epoch {epoch}: {correct}/{total}\")\n","    print(f\"Test set accuracy after epoch {epoch}: {correct/total*100:.2f}%\")"],"metadata":{"id":"Vo9RjGclcgNU","collapsed":true},"execution_count":null,"outputs":[]}]}